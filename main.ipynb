{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e06520f",
   "metadata": {},
   "source": [
    "# Detect Damage Parts from an Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0cc12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\Hp\\Desktop\\Car damage detection\\samples\\images\\img1.png: 608x640 1 Bumper, 85.3ms\n",
      "Speed: 3.4ms preprocess, 85.3ms inference, 0.8ms postprocess per image at shape (1, 3, 608, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "model = YOLO(r\"models\\best.pt\") \n",
    "\n",
    "image_path = r\"samples\\images\\img1.png\"\n",
    "results = model(image_path)\n",
    "\n",
    "for result in results:\n",
    "    result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861e099",
   "metadata": {},
   "source": [
    "# Detect Damage Parts from a Video (Q for exit the video )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4bd7770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Bonnet, 81.0ms\n",
      "Speed: 2.5ms preprocess, 81.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 45.1ms\n",
      "Speed: 3.1ms preprocess, 45.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 43.5ms\n",
      "Speed: 2.4ms preprocess, 43.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 47.5ms\n",
      "Speed: 2.8ms preprocess, 47.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Door, 47.4ms\n",
      "Speed: 2.5ms preprocess, 47.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Door, 46.7ms\n",
      "Speed: 2.7ms preprocess, 46.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 41.4ms\n",
      "Speed: 1.8ms preprocess, 41.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Light, 38.6ms\n",
      "Speed: 1.7ms preprocess, 38.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Door, 1 Light, 33.3ms\n",
      "Speed: 1.4ms preprocess, 33.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Door, 1 Light, 35.0ms\n",
      "Speed: 1.3ms preprocess, 35.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Door, 33.2ms\n",
      "Speed: 1.4ms preprocess, 33.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 1 Door, 1 Light, 34.2ms\n",
      "Speed: 1.3ms preprocess, 34.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Door, 37.0ms\n",
      "Speed: 1.7ms preprocess, 37.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 1 Door, 35.2ms\n",
      "Speed: 1.3ms preprocess, 35.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 33.0ms\n",
      "Speed: 1.5ms preprocess, 33.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Door, 1 Light, 33.9ms\n",
      "Speed: 1.6ms preprocess, 33.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 1 Door, 34.0ms\n",
      "Speed: 1.6ms preprocess, 34.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Door, 35.1ms\n",
      "Speed: 1.5ms preprocess, 35.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Door, 32.6ms\n",
      "Speed: 1.7ms preprocess, 32.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Door, 33.8ms\n",
      "Speed: 1.5ms preprocess, 33.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 35.7ms\n",
      "Speed: 1.7ms preprocess, 35.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 35.8ms\n",
      "Speed: 1.2ms preprocess, 35.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 33.9ms\n",
      "Speed: 1.3ms preprocess, 33.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Light, 33.7ms\n",
      "Speed: 1.6ms preprocess, 33.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Door, 1 Light, 52.8ms\n",
      "Speed: 4.9ms preprocess, 52.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Door, 33.4ms\n",
      "Speed: 1.6ms preprocess, 33.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Fender, 37.7ms\n",
      "Speed: 1.9ms preprocess, 37.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 36.5ms\n",
      "Speed: 1.3ms preprocess, 36.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 33.6ms\n",
      "Speed: 2.1ms preprocess, 33.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 1 Door, 36.1ms\n",
      "Speed: 2.5ms preprocess, 36.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 35.3ms\n",
      "Speed: 2.0ms preprocess, 35.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 34.6ms\n",
      "Speed: 1.2ms preprocess, 34.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 2 Bumpers, 1 Door, 1 Light, 36.9ms\n",
      "Speed: 2.0ms preprocess, 36.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Bonnets, 1 Bumper, 1 Door, 37.9ms\n",
      "Speed: 1.7ms preprocess, 37.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 2 Lights, 40.1ms\n",
      "Speed: 1.4ms preprocess, 40.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Light, 35.9ms\n",
      "Speed: 1.7ms preprocess, 35.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 1 Door, 1 Light, 34.2ms\n",
      "Speed: 1.5ms preprocess, 34.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Light, 35.3ms\n",
      "Speed: 1.8ms preprocess, 35.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Light, 32.5ms\n",
      "Speed: 1.5ms preprocess, 32.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 33.7ms\n",
      "Speed: 1.7ms preprocess, 33.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 1 Light, 35.1ms\n",
      "Speed: 1.3ms preprocess, 35.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 1 Door, 30.9ms\n",
      "Speed: 2.0ms preprocess, 30.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Bonnets, 32.5ms\n",
      "Speed: 1.7ms preprocess, 32.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 30.8ms\n",
      "Speed: 1.6ms preprocess, 30.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 34.6ms\n",
      "Speed: 1.8ms preprocess, 34.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 Bonnets, 1 Bumper, 35.1ms\n",
      "Speed: 1.6ms preprocess, 35.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "model = YOLO(r\"models\\best.pt\") \n",
    "\n",
    "video_path = r\"samples\\Video.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)  \n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS)) \n",
    "\n",
    "out = cv2.VideoWriter(\"output.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "display_width = 1200  \n",
    "display_height = 750\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break \n",
    "    \n",
    "    results = model(frame)\n",
    "    \n",
    "    for result in results:\n",
    "        annotated_frame = result.plot()\n",
    "    \n",
    "    annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    resized_frame = cv2.resize(annotated_frame, (display_width, display_height))\n",
    "\n",
    "    cv2.imshow(\"YOLO Video Detection\", resized_frame)\n",
    "\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()  \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d1ed4",
   "metadata": {},
   "source": [
    "# Image-Based Damage Detection and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf2bca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Door, 76.0ms\n",
      "Speed: 3.3ms preprocess, 76.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[INFO] Part: Door | Status: Repair | Confidence: 50.06%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "yolo_model = YOLO(r\"models\\best.pt\")\n",
    "\n",
    "classifier_model = load_model(\"models\\damage_classifier.h5\")\n",
    "classifier_labels = [\"Repair\", \"Replace\"]\n",
    "\n",
    "image_path = r\"samples\\images\\img2.jpeg\"\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "results = yolo_model(rgb_image)[0]\n",
    "\n",
    "for box in results.boxes:\n",
    "    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "    cls_id = int(box.cls[0])\n",
    "    part_label = yolo_model.names[cls_id]\n",
    "\n",
    "    cropped = rgb_image[y1:y2, x1:x2]\n",
    "    if cropped.size == 0:\n",
    "        continue\n",
    "\n",
    "    resized = cv2.resize(cropped, (224, 224))\n",
    "    input_array = img_to_array(resized) / 255.0\n",
    "    input_array = np.expand_dims(input_array, axis=0)\n",
    "\n",
    "    prediction = classifier_model.predict(input_array, verbose=0)\n",
    "    label_idx = np.argmax(prediction)\n",
    "    status_label = classifier_labels[label_idx]\n",
    "    status_conf = float(np.max(prediction))\n",
    "\n",
    "    print(f\"[INFO] Part: {part_label} | Status: {status_label} | Confidence: {status_conf * 100:.2f}%\")\n",
    "\n",
    "    color = (0, 255, 0) if status_label == \"Repair\" else (0, 0, 255)\n",
    "\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "    label_text = f\"{part_label} | {status_label} ({status_conf:.2f})\"\n",
    "\n",
    "    text_size, _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "    text_width, text_height = text_size\n",
    "\n",
    "    label_y = y1 - 10 if y1 - 10 > text_height else y1 + text_height + 10\n",
    "\n",
    "    cv2.rectangle(image, (x1, label_y - text_height - 5), (x1 + text_width + 5, label_y + 5), color, -1)\n",
    "    cv2.putText(image, label_text, (x1 + 2, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "resized = cv2.resize(image, (1200, 750))\n",
    "cv2.imshow(\"Full Image with Damage Prediction\", resized)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63e6fd8",
   "metadata": {},
   "source": [
    "# Video-Based Damage Detection and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba0bf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 44.5ms\n",
      "Speed: 2.1ms preprocess, 44.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[INFO] Part: Bumper | Status: Replace | Confidence: 98.39%\n",
      "[INFO] Part: Bonnet | Status: Replace | Confidence: 99.73%\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 42.2ms\n",
      "Speed: 2.5ms preprocess, 42.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[INFO] Part: Bonnet | Status: Replace | Confidence: 99.65%\n",
      "[INFO] Part: Bumper | Status: Replace | Confidence: 99.17%\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 42.2ms\n",
      "Speed: 1.5ms preprocess, 42.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[INFO] Part: Bonnet | Status: Replace | Confidence: 99.71%\n",
      "[INFO] Part: Bumper | Status: Replace | Confidence: 98.64%\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 37.6ms\n",
      "Speed: 1.6ms preprocess, 37.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[INFO] Part: Bumper | Status: Replace | Confidence: 98.91%\n",
      "[INFO] Part: Bonnet | Status: Replace | Confidence: 99.68%\n",
      "\n",
      "0: 384x640 1 Bumper, 35.5ms\n",
      "Speed: 2.2ms preprocess, 35.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[INFO] Part: Bumper | Status: Replace | Confidence: 98.74%\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 34.8ms\n",
      "Speed: 2.4ms preprocess, 34.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[INFO] Part: Bumper | Status: Replace | Confidence: 99.01%\n",
      "[INFO] Part: Bonnet | Status: Replace | Confidence: 99.72%\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 35.7ms\n",
      "Speed: 1.6ms preprocess, 35.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[INFO] Part: Bumper | Status: Replace | Confidence: 99.11%\n",
      "[INFO] Part: Bonnet | Status: Replace | Confidence: 99.85%\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 35.7ms\n",
      "Speed: 2.1ms preprocess, 35.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[INFO] Part: Bonnet | Status: Replace | Confidence: 99.87%\n",
      "[INFO] Part: Bumper | Status: Replace | Confidence: 99.04%\n",
      "\n",
      "0: 384x640 1 Bonnet, 1 Bumper, 1 Light, 32.9ms\n",
      "Speed: 2.0ms preprocess, 32.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[INFO] Part: Bumper | Status: Replace | Confidence: 99.12%\n",
      "[INFO] Part: Bonnet | Status: Replace | Confidence: 99.86%\n",
      "[INFO] Part: Light | Status: Replace | Confidence: 98.54%\n",
      "\n",
      "0: 384x640 1 Bumper, 35.8ms\n",
      "Speed: 2.5ms preprocess, 35.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[INFO] Part: Bumper | Status: Replace | Confidence: 99.04%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "yolo_model = YOLO(r\"models\\best.pt\")\n",
    "\n",
    "classifier_model = load_model(\"models\\damage_classifier.h5\")\n",
    "classifier_labels = [\"Repair\", \"Replace\"]\n",
    "\n",
    "video_path = r\"samples\\Video.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = yolo_model(rgb_frame)[0]\n",
    "\n",
    "    for box in results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        cls_id = int(box.cls[0])    \n",
    "        part_label = yolo_model.names[cls_id]\n",
    "\n",
    "        cropped = rgb_frame[y1:y2, x1:x2]\n",
    "        if cropped.size == 0:\n",
    "            continue\n",
    "\n",
    "        resized = cv2.resize(cropped, (224, 224))\n",
    "        input_array = img_to_array(resized) / 255.0\n",
    "        input_array = np.expand_dims(input_array, axis=0)\n",
    "\n",
    "        prediction = classifier_model.predict(input_array, verbose=0)\n",
    "        label_idx = np.argmax(prediction)\n",
    "        status_label = classifier_labels[label_idx]\n",
    "        status_conf = float(np.max(prediction))\n",
    "        \n",
    "        \n",
    "        print(f\"[INFO] Part: {part_label} | Status: {status_label} | Confidence: {status_conf * 100:.2f}%\")\n",
    "\n",
    "        color = (0, 255, 0) if status_label == \"Repair\" else (0, 0, 255)\n",
    "        label_text = f\"{part_label} | {status_label} ({status_conf:.2f})\"\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        text_size, _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "        text_width, text_height = text_size\n",
    "        label_y = y1 - 10 if y1 - 10 > text_height else y1 + text_height + 10\n",
    "\n",
    "        cv2.rectangle(frame, (x1, label_y - text_height - 5), (x1 + text_width + 5, label_y + 5), color, -1)\n",
    "        cv2.putText(frame, label_text, (x1 + 2, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    output = cv2.resize(frame, (1200, 750))\n",
    "    cv2.imshow(\"Video Damage Detection\", output)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
